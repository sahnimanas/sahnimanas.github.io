[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a CS Masters student at Georgia Tech, and a Mathematics \u0026amp; Computing graduate from Delhi College of Engineering.\nFrom 2017-2019, I was a Sotfware Engineer at Samsung Research - Bangalore, where I worked on the intersection of machine learning \u0026amp; systems, to make heavy neural networks run efficiently on low-power devices.\nI like to learn new things \u0026amp; explain them, and talk people’s ears off about work I find interesting.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sahnimanas.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a CS Masters student at Georgia Tech, and a Mathematics \u0026amp; Computing graduate from Delhi College of Engineering.\nFrom 2017-2019, I was a Sotfware Engineer at Samsung Research - Bangalore, where I worked on the intersection of machine learning \u0026amp; systems, to make heavy neural networks run efficiently on low-power devices.\nI like to learn new things \u0026amp; explain them, and talk people’s ears off about work I find interesting.","tags":null,"title":"Manas Sahni","type":"authors"},{"authors":[""],"categories":[],"content":" On my not-too-shabby laptop CPU, I can run most common CNN models in (at most) 10-100 milliseconds, with libraries like TensorFlow. In 2019, even a smartphone can run \u0026ldquo;heavy\u0026rdquo; CNN models (like ResNet) in less than half a second. So imagine my surprise when I timed my own simple implementation of a convolution layer and found that it took over 2 seconds for a single layer!\nIt\u0026rsquo;s no surprise that modern deep-learning libraries have production-level, highly-optimized implementations of most operations. But what exactly is the black magic that these libraries use that we mere mortals don\u0026rsquo;t? How are they able to improve performance by 100x? What exactly does one do to \u0026ldquo;optimize\u0026rdquo; or accelerate neural networks operations? These are questions I often asked (and get asked) when talking about high-performance/efficient DNNs.\nIn this post, I\u0026rsquo;ll attempt to walk you through how a convolution layer is implemented in DNN libraries. Not only is it one of the most common and heaviest operations in many DNN models, I also find convolution to be particularly representative of the kind of tricks that go into these high-performance implementations \u0026ndash; a little of bit of algorithmic cleverness and a lot of careful tuning and exploitation of low-level architecture.\nA lot of what I cover here is from the seminal paper \u0026ldquo;Anatomy of a high-performance matrix multiplication\u0026rdquo; by Goto et al. which formed the basis for the algorithms used in linear algebra libraries like OpenBLAS; and these helpful tutorial from Dr. Stefan Hadjis and Chris Rose.\nNaive Convolutions  \u0026ldquo;Premature optimization is the root of all evil\u0026rdquo; \u0026ndash; Donald Knuth\n Before we go around optimizing things, let\u0026rsquo;s first get a handle on our baseline and bottlenecks. Here\u0026rsquo;s a naive numpy/for-loop convolution that you\u0026rsquo;d write in Deep Learning 101:\n''' Convolve `input` with `kernel` to generate `output` input.shape = [input_channels, input_height, input_width] kernel.shape = [num_filters, input_channels, kernel_height, kernel_width] output.shape = [num_filters, output_height, output_width] ''' for filter in 0..num_filters for channel in 0..input_channels for out_h in 0..output_height for out_w in 0..output_width for k_h in 0..kernel_height for k_w in 0..kernel_width output[filter, channel, out_h, out_h] += kernel[filter, channel, k_h, k_w] * input[channel, out_h + k_h, out_w + k_w]  Yikes. That\u0026rsquo;s 6 nested for loops for one conv (7 if you iterate over batches of multiple inputs). And we\u0026rsquo;re not yet looking at stride, dilation, or any other parameters. If I plug in here the sizes for, say the first layer of MobileNet, and run this in plain C, it takes a whopping 22 seconds! With the most aggressive compiler optimizations like -O3 or -Ofast, it reduces to 2.2 seconds. But that\u0026rsquo;s still terribly slow for just the first layer.\nWhat if I run the same layer using, say, Caffe? It took just 18ms on the same PC. That\u0026rsquo;s more than a 100x speedup! The entire network itself runs in ~100ms on my CPU.\nWhat is the bottleneck and where should we begin to optimize?\nThe inner-most loop does 2 floating point operations (multiply \u0026amp; add), and for the sizes I used, it\u0026rsquo;s executed about 85.16 million times, i.e. this convolution requires 170 million floating point operations (MFLOPs). The peak performance of my CPU, according to Intel, is 80 billion FLOPs per second, i.e. it could theoretically finish the job in 0.002 seconds. Clearly, we\u0026rsquo;re nowhere near that and clearly, the raw processing capability is more than sufficient here.\nThe reason that the theoretical peak isn\u0026rsquo;t achieved (ever) is that memory accesses also take time \u0026ndash; it\u0026rsquo;s not enough to process data quickly, if you can\u0026rsquo;t get the data quickly. It turns out that the heavily-nested for-loops above make for very difficult data access patterns, which poorly utilize the cache.\nAs you\u0026rsquo;ll see, a recurring concern throughout the discussion will be how we\u0026rsquo;re accessing the data we\u0026rsquo;re operating on, and how that relates to the way it\u0026rsquo;s stored. Some Prerequisites FLOP/s Our metric for \u0026ldquo;performance\u0026rdquo; or speed will be the throughput, measured in the number of FLoating Point Operations per Second, or FLOP/s. A bigger operation with more floating-point operations will naturally run slower, so FLOP/s rate allows a more consistent way to compare performance.\nWe can also use this to get an idea of how close we are to the peak performance of the CPU. On my laptop CPU:\n there are 2 phsyical cores each core has a frequency of 2.5 GHz, or $2.5\\times10^9$ CPU cycles per second in each cycle, it can process 32 FLOPs (using AVX \u0026amp; FMA, more on this in a bit)   This gives a peak performance of $2\\times2.5\\times10^9\\frac{cycles}{second}\\times32\\frac{FLOPs}{cycle}=160$ GFLOP/s. This is the theoretical peak of my CPU. Similarly, for a single core, this number is 80 GFLOP/s.\nStorage orders and Row Major While we logically view matrices/images/tensors as multi-dimensional, they\u0026rsquo;re physically stored in a linear, one-dimensional computer memory. We have to define a convention which dictates how to unwrap these multiple dimensions to a linear storage, and vice versa.\nMost modern DL libraries use a row-major storage order. This means that consecutive elements of the same row are stored next to each other. More generally with multiple dimensions, row-major means the first dimension changes the slowest as you scan the memory linearly.\nWhat about the ordering of the dimensions themselves? Usually for 4-dimensional tensors like in CNNs, you\u0026rsquo;ll hear of storage orders like NCHW, NHWC, etc. I\u0026rsquo;ll assume NCHW throughout this post \u0026ndash; if I have N blocks of C channels of HxW images, then all images with the same N are contigous; within that block all pixels of the same channel C are contigous, and so on.\n  Halide Many of the optimizations discussed here require meddling at the lower-level with cryptic C syntax or even assembly. Not only does this make code difficult to read, it also makes trying out different optimizations difficult as we have to re-write our entire code. Halide is an embedded language in C++ that helps abstract these concepts and is designed to help write fast image-processing code. It makes it easier to experiment with different optimizations by disentangling the algorithm (what to compute) and the schedule (how/when to compute it). We can keep the algorithm fixed, and play around with different schedules.\nI\u0026rsquo;ll use Halide to represent these lower-level concepts, but you should be able to understand the intuitive function names enough to follow along.\nFrom Convolution to GEMM The naive convolution that we discussed above is slow already, and a more realistic implementation will only be further complicated by parameters like stride, dilation, padding, etc. We could continue using the basic conv as a working example but, as you\u0026rsquo;ll see, extracting maximum performance out of the computer requires many tricks \u0026ndash; carefully fine-tuning at multiple levels \u0026amp; exploting very specific knowledge of the computer architecture at hand. In other words, this is going to be a formidable task if we\u0026rsquo;re hoping to address all of the complexities.\nCan we instead transform this into a problem that\u0026rsquo;s easier to solve? Perhaps matrix multiplication?\nMatrix multiplication, or matmul, or Generalized Matrix Multiplication (GEMM), is at the heart of deep learning. It\u0026rsquo;s used in fully-connected layers, RNNs, etc., and can be used to implement convolutions too.\nConv is, after all, a dot-product of the filter with input patches. If we lay out the filter into a 2-D matrix and the input patches in another, then the multiplying these 2 matrices would compute the same dot product. And matrix multiplication \u0026ndash; unlike CNNs \u0026ndash; has been heavily studied and optimized over several decades, being a critical problem in many scientific domains.\nThe above laying out of the image patches into a matrix is called im2col, for image to column. We rearrange the image into columns of a matrix, so that each column corresponds to one patch where the conv filter is applied.\nConsider this normal, direct 3x3 convolution:  \nBelow is the same operation implemented as a matrix multiplication. The right matrix is the result of im2col \u0026ndash; it has to be constructed by copying pixels from the original image. The left matrix has the conv weights, which are already stored this way in memory.  \nNote that the matrix product gives us the conv output directly \u0026ndash; there is no need for an extra \u0026ldquo;conversion\u0026rdquo; to the original form.\nI\u0026rsquo;ve shown each patch as independent here for visual clarity. However, in reality, there usually is some overlap between different image patches, and hence im2col incurs some memory duplication. The time taken to generate this im2col buffer and the inflated memory, will have to be offset by the speedup we achieve via GEMM.\nWith im2col, we have now transformed the convolution operation into a matrix multiplication. We can now plug in more general-purpose \u0026amp; popular linear algebra libraries like OpenBLAS, Eigen, etc. to take care of efficiently computing this matmul, riding on the back of decades of optimizations \u0026amp; careful fine-tuning.\nWe\u0026rsquo;ll need some pretty serious speedups if we\u0026rsquo;re going to justify the extra work and memory resulting from the im2col transform, so let\u0026rsquo;s see how these libraries might be achieving that. This also gives a good intro to some general approaches when optimizing at the system level.\n While this idea of convolution-as-GEMM existed in different forms before, Caffe was one of the first deep-learning libraries to use this method for general convs on CPU \u0026amp; GPU and show a major speedup. Here\u0026rsquo;s a very interesting read from Yanqing Jia himself (creator of Caffe) on the origins of this decision from a thesis deadline, and thoughts on \u0026ldquo;temporary\u0026rdquo; solutions.   Speeding up the GEMM Naive Through the rest of this post, I\u0026rsquo;ll assume the GEMM is performed as $C_{M \\times N} \\mathrel{+}= A_{M \\times K} * B_{K \\times N}$ As before, first let\u0026rsquo;s time the basic, textbook matrix multiplication:\nfor i in 0..M: for j in 0..N: for k in 0..K: C[i, j] += A[i, k] * B[k, j]  Or in Halide:\nHalide::Buffer C, A, B; Halide::Var x, y; C(x,y) += A(k, x) *= B(y, k); // loop bounds, dims, etc. are taken care of automatically  The inner most line does 2 floating point ops (multiply \u0026amp; add) and is performed $M*N*K$ times, so the number of FLOPs for this GEMM is $2MNK$.\nLet\u0026rsquo;s measure its performance for various matrix sizes:  \nWe barely reach 10% of peak performance! While we\u0026rsquo;ll look into ways to make the computation faster, a recurring theme will be that it\u0026rsquo;s not enough to just compute the data fast if we can\u0026rsquo;t get the data fast. As memory becomes a bigger and bigger issue for larger matrices, the performance continues to gradually dip. The sharp drop you see towards the end? That represents the point when the matrices become too big to fit in the cache and the throughput suddenly drops \u0026ndash; you can see the system choking.\nCaching The RAM is a large but slow storage. CPU caches are orders of magnitude faster, but much smaller, so using them correctly is critical. But there is no explicit instruction that says \u0026ldquo;load this data to cache\u0026rdquo;. It\u0026rsquo;s a process automatically managed by the CPU.\nEvery time we fetch data from the main memory, the CPU automatically loads it and its neighboring memory into the cache, hoping to utilize locality of reference.\n  The first thing that you should then notice is the pattern in which we\u0026rsquo;re accessing our data. We\u0026rsquo;re traversing row-wise on $A$ and column-wise on $B$.  \nTheir storage is also row-major, so once we find A[i, k], the next element in the row, A[i, k+1] is already cached. Cool. But see what happens for $B$ :\n the next element of the column isn\u0026rsquo;t present in the cache \u0026ndash; we get a cache miss and the CPU stalls while the data is fetched once fetched, the cache also gets filled with other elements in the same row of $B$. We won\u0026rsquo;t actually use them, so they\u0026rsquo;ll be evicted soon. A few iterations later when they\u0026rsquo;re actually needed, we\u0026rsquo;ll be working to fetch them again. We\u0026rsquo;re polluting the cache with values we don\u0026rsquo;t need.    We need to rework our loops to exploit this caching ability instead. If data is being read, we might as well make use of it. This brings us to the first change we\u0026rsquo;ll make: loop reordering.\nLet\u0026rsquo;s reorder the loops from i,j,k to i,k,j:\nfor i in 0..M: for k in 0..K: for j in 0..N:  Our answer is still correct because the order of multiplications/additions doesn\u0026rsquo;t matter. The traversal order will now look like this\n  This simple change of just reordering the loops gives a considerable speedup:\n  Tiling To further improve upon reordering, there\u0026rsquo;s one more caching issue we need to consider.\nFor each row of $A$, we were looping over the entirety of $B$. With each step over B, we\u0026rsquo;ll load some of its new columns and evict some older ones from the cache. When we get to the next row of A, we start all over again from the first columns. We\u0026rsquo;re repeatedly adding and removing the same data from the cache, or thrashing it.\nThe thrashing wouldn\u0026rsquo;t occur if all our data could fit in the cache. If only we were working with smaller matrices, they could happily live together without getting evicted repeatedly. Thankfully for us, we can break down matrix multiplication over submatrices. To compute a small $r\\times c$ tile of $C$, we only need $r$ rows of $A$ and $c$ columns of $B$. Let\u0026rsquo;s break $C$ into tiles of 6x16.\nC(x, y) += A(k, y) * B(x, k); C.update() .tile(x, y, xo, yo, xi, yi, 6, 16) /* in pseudocode: for xo in 0..N/16: for yo in 0..M/6: for yi in 6: for xi in 0..16: for k in 0..K: C(...) = ... */  We\u0026rsquo;ve broken the x,y dimensions into an outer xo,yo and inner xi,yi. We\u0026rsquo;ll spend our efforts optimizing a micro-kernel for the smaller 6x16 block (marked by xi,yi), and run that micro-kernel over all the blocks (iterated by xo,yo).\nVectorization \u0026amp; FMA Most modern CPUs support SIMD, or Single Instruction Multiple Data. As the name suggests, SIMD can be used to do the same operation/instruction (like add, multiply, etc.) on multiple values simultaneously, in the same CPU cycle. If we can run SIMD instructions on say 4 data points at a time, that\u0026rsquo;s a 4x speedup straightaway.  \nSo when we calculated the peak speed of the processor, we sort of cheated and were instead referring to this vectorized performance. This is of great use for data like vectors, where we have to apply the same instruction to every vector element. But we still have to design our kernel to exploit this properly.\nThe second \u0026ldquo;hack\u0026rdquo; we used while calculating the peak FLOPs was FMA \u0026ndash; Fused Multiply-Add. While multiplying and adding are counted as 2 separate floating-point operations, they\u0026rsquo;re so common that dedicated hardware units are available that fuse the 2 and perform them as a single instruction. Using this is usually handled by the compiler.\nOn Intel CPUs, we can use SIMD (called AVX \u0026amp; SSE) for processing up to 8 floating-point numbers in a single instruction. Compiler optimizations will often be able to identify vectorization opportunities on their own, but we\u0026rsquo;ll take things in our own hands just to be sure.\nC.update() .tile(x, y, xo, yo, xi, yi, 6, 16) .reorder(xi, yi, k, xo, yo) .vectorize(xi, 8) /* in pseudocode: for xo in 0..N/16: for yo in 0..M/6: for k in 0..K: for yi in 6: for vectorized xi in 0..16: C(...) = ... */    Threading Up until now we\u0026rsquo;ve only been using one CPU core. We have multiple cores available, and each core can physically execute multiple instructions at the same time. A program can divide itself into multiple threads, and each thread can run on a separate core.\nC.update() .tile(x, y, xo, yo, xi, yi, 6, 16) .reorder(xi, yi, k, xo, yo) .vectorize(xi, 8) .parallel(yo) /* in pseudocode: for xo in 0..N/16 in steps of 16: for parallel yo in steps of 6: for k in 0..K: for yi in 6: for vectorized xi in 0..16 in steps of 8: C(...) = ... */  You might notice that the performance actually drops for very small sizes, because with small workloads, the threads spend less time working and more time synchronizing with each other. There are a lot of other such issues with respect to threading that could warrant another deep dive on its own.\n  Unrolling Loops let us avoid the pain of writing the same line over and over again, while introducing a little extra work like checking for loop termination, updating the loop counters, pointer arithmetic, etc. If instead we manually write out the repeated loop statements and unroll the loop, we can reduce this overhead. For instance, instead of 8 iterations of 1 statement, we could run 2 iterations of 4 statements.\nI initially found it surprising that such fickle and seemingly insignificant overheads can actually matter. However, while these loop operations may be \u0026ldquo;inexpensive\u0026rdquo;, they\u0026rsquo;re certainly not free. The cost of 2-3 extra instructions for each iteration will add up quickly if you remember that the number of iterations here is in millions. The benefits do diminish progressively as the loop overhead becomes relatively smaller.\nUnrolling is another optimization that is almost entirely taken care of by compilers now, except in micro-kernels like ours where we prefer more control.\nC.update() .tile(x, y, xo, yo, xi, yi, 6, 16) .reorder(xi, yi, k, xo, yo) .vectorize(xi, 8) .unroll(xi) .unroll(yi) /* in pseudocode: for xo in 0..N/16: for parallel yo: for k in 0..K: C(xi:xi+8, yi+0) C(xi:xi+8, yi+1) ... C(xi:xi+8, yi+5) C(xi+8:xi+16, yi+0) C(xi+8:xi+16, yi+1) ... C(xi+8:xi+16, yi+5) */  We\u0026rsquo;re now able to touch speeds up to 60 GFLOP/s.  \nPutting It together The above steps cover some of the most commonly-used transformations to speed up performance. They\u0026rsquo;re usually combined in different ways to come up with more and more complex schedules to compute the same task.\nHere\u0026rsquo;s a schedule from Halide that\u0026rsquo;s more carefully optimized.\nmatrix_mul(x, y) += A(k, y) * B(x, k); out(x, y) = matrix_mul(x, y); out.tile(x, y, xi, yi, 24, 32) .fuse(x, y, xy).parallel(xy) .split(yi, yi, yii, 4) .vectorize(xi, 8) .unroll(xi) .unroll(yii); matrix_mul.compute_at(out, yi) .vectorize(x, 8).unroll(y); matrix_mul.update(0) .reorder(x, y, k) .vectorize(x, 8) .unroll(x) .unroll(y) .unroll(k, 2);  In summary, it does this:\n Split out into tiles of 32x24. Further split each tile into 8x24 sub-tiles Compute the 8x24 matmul in a temporary buffer (matrix_mul), with the similar reordering, vectorization, and unrolling Copy the temporary matrix_mul back to out with vectorization, unrolling, etc. Parallelize this process over all 32x24 tiles    Finally, we\u0026rsquo;re able to touch speeds of over 120 GFLOPs \u0026ndash; respectably close to the peak performance of 160 GFLOPs, and matching production-level libraries like OpenBLAS. With similar fine-tuned code for im2col, followed by gemm, the same convolution now runs in ~20ms. If you\u0026rsquo;re interested in going deeper into this, try experimenting with different schedules of your own \u0026ndash; while as an engineer I had always heard statements about caching, performance, etc., seeing their true effect can be rewarding and fun.\nNote that this im2col+gemm method is a popular general-purpose methods in most deep learning libraries. However, specialization is key \u0026ndash; for specific commonly-used sizes, different architectures (GPU), and different operation parameters (like dilation, grouping, etc.), these libraries might again have more specialized implementations utilizing similar tricks or assumptions specific to those cases. These micro-kernels are built after a highly iterative process with trial and error. Programmers often have only an intuition of what should/shouldn\u0026rsquo;t work well and/or must think of explanations based on the results. Sounds like a perfect match for deep learning research, right?\n","date":1566836839,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566836839,"objectID":"bc851de286b4608373b12dc9cba389c8","permalink":"https://sahnimanas.github.io/post/anatomy-of-a-high-performance-convolution/","publishdate":"2019-08-26T21:57:19+05:30","relpermalink":"/post/anatomy-of-a-high-performance-convolution/","section":"post","summary":"It's no surprise that modern deep-learning libraries have production-level, highly-optimized implementations of most operations. But just what is the black magic that these libraries use that we mere mortals don't? What exactly does one do to \"optimize\" or accelerate neural networks operations? ","tags":[],"title":"Anatomy of a High-Speed Convolution","type":"post"},{"authors":[""],"categories":null,"content":" This post was featured on Heartbeat by Fritz and DL Weekly  \nFrancois Chollet puts it concisely: - make it possible\n- make it work\n- make it efficient\n- make it dependable and invisible\n- move on to the next layer and never think about it again\n\u0026mdash; François Chollet (@fchollet) April 15, 2018 \nFor many deep learning problems, we\u0026rsquo;re finally starting with the “make it efficient” stage. We had been stuck at the first two stages for many decades where speed and efficiency weren\u0026rsquo;t nearly as important as getting things to work in the first place. So, the question of how precise our calculations need to be – and whether we can manage with lower precision – wasn\u0026rsquo;t often asked. However, now that neural networks are good enough at many problems to be of production-grade or better, this question has risen again. And the answers suggest we could do with low(er) precision, causing what may soon be a paradigm shift in mobile-optimized AI. This post talks about the concept of quantized inference, and how it works in TensorFlow-Lite.\nTL;DR just tell me how to quantize my model: Here\u0026rsquo;s a tutorial from TensorFlow with code\n   Table of Contents (click to expand)   Table of Contents [What is low-precision?](#what-is-low-precision)   [Why do we care?](#why-do-we-care)   [Why does it work?](#why-does-it-work)   [**Quantization in TF-Lite**](#quantization-in-tf-lite)   -- [Floating point vs Fixed-point](#floating-point-vs-fixed-point)   -- [Quantization scheme](#quantization-scheme)   -- [A typical quantized layer](#a-typical-quantized-layer)   -- [\"Fake\" Quantization](#%22fake%22-quantization)   -- [What's next](#whats-next)   [Further Reading](#further-reading)  -- \nWhat is low-precision? Computers can only use a finite number of bits to represent infinite real numbers. How accurately we can represent them is decided by how many bits we use – with 32-bit floating point being the default for most applications, including deep learning. It turns out that DNNs can work with smaller datatypes, with less precision, such as 8-bit integers. Roughly speaking, we\u0026rsquo;re trying to work with a number line looking closer to the sparse one on the bottom. The numbers are quantized, i.e. discretized to some specific values, which we can then represent using integers instead of floating-point numbers. To be more precise (no pun), we\u0026rsquo;ll use 8-bit fixed-point representation, which I\u0026rsquo;ll get back to in a while.\n  Why do we care? Supporting inference with quantized types in any ML framework like Caffe, TensorFlow, etc. would require us to rework significant parts of the library\u0026rsquo;s design, as well as re-implement most layers. Yet, there are several reasons that make the gains worth this effort:\n Arithmetic with lower bit-depth is faster, assuming the hardware supports it. Even though floating-point computation is no longer “slower” than integer on modern CPUs, operations with 32-bit floating point will almost always be slower than, say, 8-bit integers. In moving from 32-bits to 8-bits, we get (almost) 4x reduction in memory straightaway. Lighter deployment models mean they hog lesser storage space, are easier to share over smaller bandwidths, easier to update, etc. Lower bit-widths also mean that we can squeeze more data in the same caches/registers. This means we can reduce how often we access things from RAM, which is usually consumes a lot of time and power. Floating point arithmetic is hard – which is why it may not always be supported on microcontrollers on some ultra low-power embedded devices, such as drones, watches, or IoT devices. Integer support, on the other hand, is readily available.\n  You can see why all of this sounds like great news for someone interested in deep learning applications on mobiles or embedded devices. Deep learning researchers are now finding ways to train models that work better with quantization, ML library developers are building extensive framework support for quantized inference, and tech giants are throwing their weight behind dedicated hardware for AI with emphasis on quantization support (Google, Huawei, Microsoft, Facebook, Apple… ). Even without such dedicated hardware, DSP chips on modern smartphone chipsets have instruction sets well-suited for this kind of integer computation. Why does it work? There has been an increasing amount of work in quantizing neural networks, and they broadly point to two reasons. First, DNNs are known to be quite robust to noise and other small perturbations once trained. This means even if we subtly round-off numbers, we can still expect a reasonably accurate answer. Moreover, the weights and activations by a particular layer often tend to lie in a small range, which can be estimated beforehand. This means we don\u0026rsquo;t need the ability to store 106 and 10-6 in the same data type - allowing us to concentrate our precicious fewer bits within a smaller range, say -3 to +3. As you may imagine, it\u0026rsquo;ll be crucial to accurately know this smaller range - a recurring theme you\u0026rsquo;ll see below.\nSo, if done right, quantization only causes a small loss of precision which usually doesn\u0026rsquo;t change the output significantly. Finally, small losses in accuracy can be recovered by retraining our models to adjust to quantization.\nYou can see an example below of the weights in a layer from AlexNet, with a histogram of actual weights on the left. Notice how most values lie in a small range. We can quantize, i.e. discretize the range to only record some of these values accurately, and round-off the rest. The right sub-graph shows one such quantization using 4-bits (16 discrete values). You can see how we can improve this with a less stringent bit-length of say, 8-bits.\nSource: Han et al\nWhy not train in lower precision directly, you ask? Well, it\u0026rsquo;s not impossible but we\u0026rsquo;re yet to iron out many kinks. Models are trained using very tiny gradient updates, for which we do need high precision. However, there have been a plethora of experiments with quantization \u0026ndash; we have seen results with quantization in training (1, 2, 3), or with more intricate methods that use variable-precision, methods that replace multiplications with bit-wise ops, ternary or even binary weights! However, many of them have been restricted to experimental studies, or still have ways to go from being widely applicable. For the remainder of this post, I\u0026rsquo;ll be talking about the more common task of inference using 8-bit fixed point quantization in TensorFlow Lite, as described in this paper.\nQuantization in TF-Lite Floating-point vs Fixed-point First, a quick primer on floating/fixed-point representation. Floating point uses a mantissa and an exponent to represent real values – and both can vary. The exponent allows for representing a wide range of numbers, and the mantissa gives the precision. The decimal point can “float”, i.e. appear anywhere relative to the digits.\nIf we replace the exponent by a fixed scaling factor, we can use integers to represent the value of a number relative to (i.e. an integer multiple of) this constant. The decimal point\u0026rsquo;s position is now \u0026ldquo;fixed\u0026rdquo; by the scaling factor. Going back to the number line example, the value of the scaling factor determines the smallest distance between 2 ticks on the line, and the number of such ticks is decided by how many bits we use to represent the integer (for 8-bit fixed point, 256 or 28). We can use these to tradeoff between range and precision. Any value that is not an exact multiple of the constant will get rounded to the nearest point.\nSource: Courbariaux et al\nQuantization Scheme Unlike floating point, there is no universal standard for fixed-point numbers, and is instead domain-specific. Our quantization scheme (mapping between real \u0026amp; quantized numbers) requires the following:\n1. It should be linear (or affine).\n If it weren\u0026rsquo;t that way, then the result of fixed-point calculations won\u0026rsquo;t directly map back to real numbers.  2. It allows us to always represent 0.f accurately.\n If we quantize and dequantize any real value, only 256 (or generally, 2B) of them will return the exact the same number, while all others will suffer some precision loss. If we ensure that 0.f is one of these 256 values , it turns out that DNNs can be quantized more accurately. The authors claim that this improves accuracy because 0 has a special significance in DNNs (such as padding). Besides, having 0 map to another value that\u0026rsquo;s higher/lower than zero will introduce a bias in the quantization scheme.  So our quantization scheme will simply be a shifting and scaling of the real number line to a quantized number line. For a given set of real values, we want the minimum/maximum real values in this range $\\left[r_{min}, r_{max}\\right]$ to map to the minimum/maximum integer values $[0,2^B-1]$ respectively, with everything in between linearly distributed.\nThis gives us a pretty simple linear equation:\n$$ \\begin{aligned} r \u0026amp;= {r_{max}-r_{min} \\over (2^B-1) - 0} \\times (q-z) \\\\\n\u0026amp;= S \\times (q-z) \\end{aligned} $$\nHere,\n $r$ is the real value (usually float32) $q$ is its quantized representation as a $B$-bit integer (uint8, uint32, etc.) $S$ (float32) and $z$ (uint) are the factors by which we scale and shift the number line. $z$ will always map back exactly to 0.f.    From this point, we\u0026rsquo;ll assume quantized variables to be represented as uint8, except where mentioned. Alternatively, we could also use int8, which would just shift the zero-point, $z$.\nThe set of numbers being quantized with the same parameters are values we expect to lie in the same range, such as weights of a given layer or activation outputs at a given node. We\u0026rsquo;ll see later how to find the actual ranges for various quantities in TensorFlow\u0026rsquo;s fake quantization nodes. First, let\u0026rsquo;s see just put this together to see how these quantized layers fit in a network. \nA typical quantized layer Let\u0026rsquo;s look at the components of a conventional layer implemented in floating-point:\n Zero or more weight tensors, which are constant, and stored as float. One or more input tensors; again, stored in float. The forward pass function which operates on the weights and inputs, using floating point arithmetic, storing the output in float Output tensors, again in float.  Now the weights of a pre-trained network are constant, so we can convert \u0026amp; store them in quantized form beforehand with their exact ranges known to us.\nThe input to a layer, or equivalently the output of a preceding layer, are also quantized with their own separate parameters. But wait – to quantize a set of numbers don\u0026rsquo;t we need to know their range (and thus their actual values) in float first? Then what\u0026rsquo;s the point of quantized computation? The answer to this lies behind the fact that a layer\u0026rsquo;s output generally lies in a bounded range for most inputs, with only a few outliers. While we ideally would want to know the exact range of values to quantize them accurately, results of unknown inputs can still be expected to be in similar bounds. Luckily, we are already computing the output in float during another stage – training. Thus, we can find the average output range on a large number of inputs during training and use this as a proxy to the output quantization parameters. When running on an actual unseen input, an outlier will get squashed if our range is too small, or get rounded if the range is too wide. But hopefully there will only be a few of these.\nWhat\u0026rsquo;s left is the main function that computes the output of the layer. Changing this to a quantized version requires more than simply changing float to int everywhere, as the results of our integer computations can overflow. So, we\u0026rsquo;ll have to store results in larger integers (say, int32) and then requantize it to the 8-bit output. This is not a concern in conventional full-precision implementations, where all variables are in float and the hardware handles all the nitty-gritties of floating-point arithmetic. Additionally, we\u0026rsquo;ll also have to change some of the layers\u0026rsquo; logic. For example, ReLU should now compare values against Quantized(0) instead of 0.f\nThe below figure puts it all together.\nWe can even get a bit clever with the re-quantization in (3). TF-Lite uses gemmlowp for matrix multiplication, which stores results of uint8 matrix products in int32. Then, we can add the biases quantized in higher precision as int32 itself. Finally, in going from 32-bit to 8-bit, (4) would expect the range of this layer\u0026rsquo;s output. Instead, we can specify the quantization range expected after the next activation layer, such as ReLU. This will implicitly compute activations and also help us use the full quantization range in this layer.\n\n\u0026ldquo;Fake\u0026rdquo; Quantization Now that we have everything in place to work with quantized variables, what\u0026rsquo;s left is preparing \u0026amp; converting a conventional neural network to the quantized form, which is where TensorFlow\u0026rsquo;s “fake quantization” nodes come in.\n1. The first role that they fulfil is making the network more immune to precision loss due to quantization.\n The simplest approach to quantizing a neural network is to first train it in full precision, and then simply quantize the weights to fixed-point. This approach works okay for large models, but with small models with less redundant weights, the loss in precision adversely affects accuracy. With the fake quantization nodes, the rounding effect of quantization is simulated in the forward pass as it would occur in actual inference. In a way, we\u0026rsquo;re looking to fine-tune the weights to adjust for the precision loss. All quantities are still stored as float with full-precision desirable during training, and backpropagation still works as usual.  2. Secondly, fake quantization nodes record the ranges of activations during training, which we discussed earlier.\n These nodes are placed in the training graph to exactly match wherever activations would change quantization ranges (input and output in below figure). As the network trains, they collect a moving average of the ranges of float values seen at that node.      \nSource: Benoit et al\nAll this information is then taken by TF-Lite\u0026rsquo;s TOCO (TensorFlow Optimizing COnverter) tool which – apart from other optimizations – performs the actual conversion to quantized values and specifies how to use them in inference by TF-Lite\u0026rsquo;s kernels on mobile devices.\nThe chart below shows the accuracy-latency tradeoff for various MobileNet models for ImageNet classification in quantized and float inference modes. For most part, the whole quantization pipeline works well and only suffers from very minor losses in accuracy. An interesting area to explore further is how this loss can be also be recovered via retraining.\n Accuracy-latency tradeoff with MobileNets. Source: Benoit et al   What\u0026rsquo;s next Most of the processes described here are specific to how quantization is done in TensorFlow Lite, which only deals with quantized inference with a model trained using good old single precision. Even for inference, it just happens to be one of many options and it remains to be seen if other approaches might work better. What is certain is that the benefits offered by quantization today on mobile devices are real, and perhaps beyond mobile devices in the future; and hence the field is seeing increasing interest from all sorts of stakeholders. There are all kinds of other results with quantized training, non-linear quantization, binary quantization, networks without multipliers… it\u0026rsquo;s a growing list, which I hope to cover soon.\nFurther Reading Quantization in TF-Lite \n Pete Warden\u0026rsquo;s blog posts on quantization: 1, 2, 3  Jacob B, Kligys S, Chen B, Zhu M, Tang M, Howard A, Adam H, Kalenichenko D. \u0026ldquo;Quantization and training of neural networks for efficient integer-arithmetic-only inference.\u0026rdquo; arXiv preprint arXiv:1712.05877. 2017 Dec 15.  Quantized training \n Li H, De S, Xu Z, Studer C, Samet H, Goldstein T. \u0026ldquo;Training Quantized Nets: A Deeper Understanding.\u0026rdquo; Neural Information Processing Systems (NIPS), 2017  Gupta, Suyog, et al. \u0026ldquo;Deep learning with limited numerical precision.\u0026rdquo; International Conference on Machine Learning. 2015.  Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \u0026ldquo;Training deep neural networks with low precision multiplications.\u0026rdquo; arXiv preprint arXiv:1412.7024 (2014).  Wu, Shuang, et al. \u0026ldquo;Training and inference with integers in deep neural networks.\u0026rdquo; arXiv preprint arXiv:1802.04680 (2018).  Extremely low-bit quantization \n Zhu, Chenzhuo, et al. \u0026ldquo;Trained ternary quantization.\u0026rdquo; arXiv preprint arXiv:1612.01064 (2016).  Courbariaux, Matthieu, et al. \u0026ldquo;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.\u0026rdquo; arXiv preprint arXiv:1602.02830 (2016).  Rastegari, Mohammad, et al. \u0026ldquo;Xnor-net: Imagenet classification using binary convolutional neural networks.\u0026rdquo;European Conference on Computer Vision. Springer, Cham, 2016.  Quantization for compression \n Han, Song, Huizi Mao, and William J. Dally. \u0026ldquo;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\u0026rdquo; arXiv preprint arXiv:1510.00149 (2015).  ","date":1529798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544140800,"objectID":"43edf930f932020e186db3a1229d818e","permalink":"https://sahnimanas.github.io/post/quantization-in-tflite/","publishdate":"2018-06-24T00:00:00Z","relpermalink":"/post/quantization-in-tflite/","section":"post","summary":"Deploying efficient neural nets on mobiles is becoming increasingly important. This post explores the concept of quantized inference, and how it works in TensorFlow Lite.","tags":["quantization","tf-lite","inference","performance"],"title":"Making Neural Nets Work With Low Precision","type":"post"}]