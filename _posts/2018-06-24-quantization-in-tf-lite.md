---
layout: post
title: Making Neural Nets Work With Low Precision
tags: [quantization, tf-lite]
---
### Quantized Neural Networks & TensorFlow-Lite
{: style="text-align: center"}

<br /> 
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">- make it possible<br>- make it work<br>- make it efficient<br>- make it dependable and invisible<br>- move on to the next layer and never think about it again</p>&mdash; François Chollet (@fchollet) <a href="https://twitter.com/fchollet/status/985607164461907968?ref_src=twsrc%5Etfw">April 15, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

As Francois Chollet concisely puts it, we're currently starting with the “make it efficient” stage in deep learning. We had been stuck at the first two stages for many decades where speed and efficiency weren't nearly as important as getting things to work in the first place. So, the question of how precise our calculations need to be – and whether we can manage with lower precision – wasn't often asked. However, now that neural networks are good enough at many problems to be of production-grade or better, this question has risen again. And the answers suggest we could do with low(er) precision, causing what may soon be a paradigm shift in mobile-optimized AI. 
{: style="text-align: justify"}


What is low-precision? 
-----------------------
Computers can only use a finite number of bits to represent infinite real numbers. How accurately we can represent them is decided by how many bits we use – with 32-bit floating point being the default for most applications, including deep learning. It turns out that DNNs can work with smaller datatypes, with less precision, such as 8-bit integers. Roughly speaking, we're trying to work with a number line looking closer to the one on the right. The numbers are quantized, i.e. discretized to some specific values. To be more precise (no pun), we use 8-bit fixed-point representation, which we'll get back to in a while.
{: style="text-align: justify"}


Why do we care? 
-------------------
Supporting inference with quantized types in any library would require us to rework significant parts of the library's design, as well as implementations of most layers. Yet, there are several reasons that make the gains worth this effort: 

 * Arithmetic with lower bit-depth is faster, assuming the hardware supports it. Even though floating-point computation is no longer “slower” than integer on modern CPUs, operations with 32-bit floating point will almost always be slower than, say, 8-bit integers. 
 * In moving from 32-bits to 8-bits, we get (almost) 4x reduction in memory straightaway. Lighter deployment models mean they hog lesser storage space, are easier to share over smaller bandwidths, easier to update, etc. 
 * Lower bit-widths also mean that we can squeeze more data in the same caches/registers and reduce DRAM access, which could mean big savings in time and power. 
 * Floating point arithmetic is [hard][fp_hard_rant] – which is it may not always be supported on microcontrollers on smaller embedded devices, such as drones, watches, or other IoT. Integer support, on the other hand, is readily available.  


You can see why all of this sounds like great news for someone interested in deep learning applications on mobiles or embedded devices. Deep learning researchers are now finding ways to train models that work better with quantization, ML library developers are building extensive framework support for quantized inference, and tech giants are throwing their weight behind [dedicated hardware for AI][ai_chip_general] with emphasis on quantization support ([Google][google_tpu], [Huawei][kirin970], [Microsoft][ms_chip], [Facebook][fb_chip], [Apple][iphone_neural_engine]… ). Even without such dedicated hardware, DSP chip on modern smartphone chipsets have instruction sets well-suited for this kind of integer computation. 
{: style="text-align: justify"}

Why does it work?  
----------------------
There has been an increasing amount of work in quantizing neural networks, and they broadly point to two reasons. First, DNNs are known to be quite robust to noise and other small perturbations once trained. Moreover, the activations generated by a particular layer often tend to lie in a small range, which can be estimated beforehand. This smaller dynamic range allows us to accurately represent these numbers despite less precision. So, if done right, quantization only causes a small loss of precision which usually doesn't change the output significantly.  
{: style="text-align: justify"}

People have managed to show results with quantization even in training [[1][wage], [2][courbariaux], [3][dl_with_limited_precision]], or with more intricate quantization methods that use variable-precision, methods that replace multiplications with [bit-wise ops][xnor], [ternary][ternary] or even [binary weights][binary]! However, many of them have been restricted to experimental results or very specific hardware. For the remainder of this post, I'll be talking about the more common 8-bit fixed point quantization in TensorFlow Lite, as described in [this paper][gemmlowp_paper]. 
{: style="text-align: justify"}

***

Quantization in TF-Lite 
==========================

Floating-point vs Fixed-point
---------------------------------

First, a quick primer on floating/fixed-point representation. Floating point uses a mantissa and an exponent to represent real values – and both can vary. The exponent allows for representing a wide range of numbers, and the mantissa gives the precision. The decimal point can “float”, i.e. appear anywhere relative to the digits. 
{: style="text-align: justify"}
 
If we replace the exponent by a fixed scaling factor, we can use integers to represent the value of a number relative to this constant. The decimal point's position is now fixed by the scaling factor. Going back to the number line example, the value of the scaling factor determines the smallest distance between 2 ticks, and the number of such ticks is decided by how many bits we use to represent the integer (for 8-bit fixed point, 256 or 2<sup>8</sup>). We can use these to tradeoff between range and precision. 
{: style="text-align: justify"}

![floatfixed]
_<sub>Source: [Courbariaux et al][courbariaux]</sub>_


## Quantization Scheme 
Unlike floating point, there is no universal standard for fixed-point numbers, and is instead domain-specific. Our quantization scheme (mapping between real & quantized numbers) requires the following: 

**1. It should be linear (or affine).**
: If it weren't that way, then the result of fixed-point calculations won't directly map back to real numbers. 

**2. It allows us to always represent `0.f` accurately.**
: If we quantize and dequantize any real value, only 256 (or generally, 2<sup>B</sup>) of them will return the exact the same number, while all others will suffer some precision loss. If we ensure that `0.f` is one of these 256 values , it turns out that DNNs can be quantized more accurately. The authors claim that this improves accuracy because 0 has a special significance in DNNs (such as padding). Besides, having 0 map to another value that's higher/lower than zero will introduce a bias in the quantization scheme. 

So our quantization scheme will simply be a shifting and scaling of the real number line to a quantized number line. For a given set of real values, we want the minimum/maximum real values in this range $$[rmin,rmax]$$ to map to the minimum/maximum integer values $$[0,2^B-1]$$ respectively, with everything in between linearly distributed.\\
This gives us a pretty simple linear equation:
{: style="text-align: justify"}

$$
\begin{aligned}
  r &= {rmax-rmin \over (2^B-1) - 0} \times (q-z) \\
  &= S \times (q-z) 
\end{aligned}
$$


Here, 
- $$r$$ is the real value (usually `float32`) 
- $$q$$ is its quantized representation as a $$B$$-bit integer (`uint8`, `uint32`, etc.)
- $$S$$ (`float32`) and $$z$$ (`uint`) are the factors by which we scale and shift the number line

![numberline]

From this point, we'll assume quantized variables to be represented as `uint8`, except where mentioned. Alternatively, we could also use int8, which would just shift the zero-point, $$z$$.

The set of numbers being quantized with the same parameters are values we expect to lie in the same range, such as weights of a given layer or activation outputs at a given node. We'll see later how to find the actual ranges for various quantities in TensorFlow's *fake quantization* nodes. First, let's see just put this together to see how these quantized layers fit in a network. 
{: style="text-align: justify"}

A typical quantized layer
-----------------------------
Let's look at the components of a conventional layer implemented in floating-point:

 - Zero or more weight tensors, which are constant, and stored as float. 
 - One or more input tensors; again, stored in float. 
 - The forward pass function which operates on the weights and inputs, using floating point arithmetic, storing the output in float
 - Output tensors, again in float.

Now the weights of a pre-trained network are constant, so we can convert & store them in quantized form beforehand with their exact ranges known to us. 
{: style="text-align: justify"}

The input to a layer, or equivalently the output of a preceding layer, are also quantized with their own separate parameters. But wait – to quantize a set of numbers don't we need to know their range (and thus their actual values) in float first? Then what's the point of quantized computation? The answer to this lies behind the fact that a layer's output generally lies in a bounded range for most inputs, with only a few outliers. While we ideally would want to know the exact range of values to quantize them accurately, results of unknown inputs can still be expected to be in similar bounds. Luckily, we are already computing the output in float during another stage – training. Thus, we can find the average output range on a large number of inputs during training and use this as a proxy to the output quantization parameters. When running on an actual unseen input, an outlier will get squashed if our range is too small, or get rounded if the range is too wide. But hopefully there will only be a few of these.
{: style="text-align: justify"}

What's left is the main function that computes the output of the layer. Changing this to a quantized version requires more than simply changing float to int everywhere, as the results of our integer computations can overflow. So, we'll have to store results in larger integers (say, int32) and then requantize it to the 8-bit output. This is not a concern in conventional full-precision implementations, where all variables are in float and the hardware handles all the nitty-gritties of floating-point arithmetic. Additionally, we'll also have to change some of the layers' logic. For example, ReLU should now compare values against `Quantized(0)` instead of `0.f` 
{: style="text-align: justify"}

The below figure puts it all together. 

![layer]

We can even get a bit clever with the re-quantization in (3). TF-Lite uses gemmlowp for matrix multiplication, which stores results of `uint8` matrix products in `int32`. Then, we can add the biases quantized in higher precision as `int32` itself. Finally, in going from 32-bit to 8-bit, (4) would expect the range of this layer's output. Instead, we can specify the quantization range expected after the next activation layer, such as ReLU. This will implicitly compute activations and also help us use the full quantization range in this layer.  
{: style="text-align: justify"}
 
_Fake_ Quantization
----------------------
Now that we have everything in place to work with quantized variables, what's left is preparing & converting a 
conventional neural network to the quantized form, which is where TensorFlow's “fake quantization” nodes come in. 

**1. The first role that they fulfil is making the network more immune to precision loss due to quantization.**

:   The simplest approach to quantizing a neural network is to first train it in full precision, and then simply quantize the weights to fixed-point. This approach works okay for large models, but with small models with less redundant weights, the loss in precision adversely affects accuracy. With the fake quantization nodes, the rounding effect of quantization is simulated in the forward pass as it would occur in actual inference. In a way, we're looking to fine-tune the weights to adjust for the precision loss. All quantities are still stored as float with full-precision desirable during training, and backpropagation still works as usual. 
{: style="text-align: justify"}

**2. Secondly, fake quantization nodes record the ranges of activations during training, which we discussed earlier.**
:  These nodes are placed in the training graph to exactly match wherever activations would change quantization ranges (input and output in below figure). As the network trains, they collect a moving average of the ranges of float values seen at that node. 
{: style="text-align: justify"}

![simple]
![simple_quantized]

*<sub>Source: [Benoit et al][gemmlowp_paper]</sub>*

   
All this information is then taken by TF-Lite's TOCO (TensorFlow Optimizing COnverter) tool which – apart from other optimizations – performs the actual conversion to quantized values and specifies how to use them in inference by TF-Lite's kernels on mobile devices. 
{: style="text-align: justify"}

## What's next
Most of the processes described here are specific to how quantization is done in TensorFlow Lite, which only deals with quantized inference with a model trained using good old single precision. Even for inference, it just happens to be one of many options and it remains to be seen if other approaches might work better. What is certain is that the benefits offered by quantization today on mobile devices are real, and perhaps beyond mobile devices in the future; and hence the field is seeing increasing interest from all sorts of stakeholders. There are all kinds of other results with quantized training, non-linear quantization, binary quantization, networks without multipliers… it's a growing list, which I hope to cover soon.
{: style="text-align: justify"}

[ai_chip_general]: https://www.theverge.com/2017/10/19/16502538/mobile-ai-chips-apple-google-huawei-qualcomm
[iphone_neural_engine]: https://www.theverge.com/2017/9/13/16300464/apple-iphone-x-ai-neural-engine
[kirin970]: https://www.androidauthority.com/huawei-announces-kirin-970-797788/
[fb_chip]: https://www.theverge.com/2018/4/18/17254236/facebook-designing-own-chips-ai-report
[ms_chip]: https://www.cnet.com/news/microsoft-project-brainwave-speeds-ai-with-fpga-chips-on-azure-build-conference/
[google_tpu]: https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu

[gemmlowp_paper]: https://arxiv.org/abs/1712.05877
[dl_with_limited_precision]: https://arxiv.org/abs/1502.02551
[courbariaux]: https://arxiv.org/abs/1412.7024
[ternary]: https://arxiv.org/abs/1605.04711
[binary]: https://arxiv.org/abs/1602.02830
[xnor]: https://arxiv.org/pdf/1603.05279v1.pdf
[wage]: https://arxiv.org/pdf/1802.04680.pdf
[deep_compression]: https://arxiv.org/abs/1510.00149

[fp_hard_rant]: https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/

[floatfixed]: {{site.baseurl}}/assets/img/quantization/floatfixed.png 
[layer]: {{site.baseurl}}/assets/img/quantization/layer.png
{: width="1400px"}

[simple]: {{site.baseurl}}/assets/img/quantization/simple.png
{: width="40%"}
[simple_quantized]: {{site.baseurl}}/assets/img/quantization/simple_quantized.png
{: width="40%"}
[numberline]: {{site.baseurl}}/assets/img/quantization/numberline.png
[densesparse]: {{site.baseurl}}/assets/img/quantization/densesparse.png